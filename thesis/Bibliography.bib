% This file was created with JabRef 2.7b.
% Encoding: UTF-8

@ARTICLE{9,
  author = {Barnich, O. and Van Droogenbroeck, M.},
  title = {ViBe: A Universal Background Subtraction Algorithm for Video Sequences},
  journal = {Image Processing, IEEE Transactions on},
  year = {2011},
  volume = {20},
  pages = {1709--1724},
  number = {6},
  month = jun,
  abstract = {{This paper presents a technique for motion detection that incorporates
	several innovative mechanisms. For example, our proposed technique
	stores, for each pixel, a set of values taken in the past at the
	same location or in the neighborhood. It then compares this set to
	the current pixel value in order to determine whether that pixel
	belongs to the background, and adapts the model by choosing randomly
	which values to substitute from the background model. This approach
	differs from those based upon the classical belief that the oldest
	values should be replaced first. Finally, when the pixel is found
	to be part of the background, its value is propagated into the background
	model of a neighboring pixel. We describe our method in full details
	(including pseudo-code and the parameter values used) and compare
	it to other background subtraction techniques. Efficiency figures
	show that our method outperforms recent and proven state-of-the-art
	methods in terms of both computation speed and detection rate. We
	also analyze the performance of a downscaled version of our algorithm
	to the absolute minimum of one comparison and one byte of memory
	per pixel. It appears that even such a simplified version of our
	algorithm performs better than mainstream techniques.}},
  citeulike-article-id = {10548921},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/tip.2010.2101613},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5672785},
  doi = {10.1109/tip.2010.2101613},
  institution = {EVS Broadcast Equip., Seraing, Belgium},
  issn = {1057-7149},
  keywords = {catt},
  posted-at = {2012-09-11 18:10:17},
  priority = {2},
  publisher = {IEEE},
  url = {http://dx.doi.org/10.1109/tip.2010.2101613}
}

@INPROCEEDINGS{7,
  author = {Chaudhury, S. and Tripathi, S. and Roy, S.D.},
  title = {Parametric video compression using appearance space},
  booktitle = {Pattern Recognition, 2008. ICPR 2008. 19th International Conference
	on},
  year = {2008},
  pages = {1-4},
  abstract = {The novelty of the approach presented in this paper is the unique
	object-based video coding framework for videos obtained from a static
	camera. As opposed to most existing methods, the proposed method
	does not require explicit 2D or 3D models of objects and hence is
	general enough to satisfy the need for varying types of objects in
	the scene. The proposed system detects and tracks an object in the
	scene by learning the appearance model of each object online using
	nontraditional uniform norm based subspace. At the same time the
	object is coded using the projection coefficients to the orthonormal
	basis of the subspace learnt. The tracker incorporates a predictive
	framework based upon Kalman filter for predicting the five motion
	parameters. The proposed method shows substantially better compression
	than MPEG2 based coding with almost no additional complexity.},
  doi = {10.1109/ICPR.2008.4761652},
  issn = {1051-4651},
  keywords = {Kalman filters;cameras;data compression;object detection;video coding;Kalman
	filter;MPEG2 based coding;appearance space;object detection;object
	tracking;parametric video compression;projection coefficients;static
	camera;uniform norm based subspace;unique object-based video coding;Cameras;Encoding;Image
	coding;Image sequences;Layout;Object detection;Tracking;Transform
	coding;Video coding;Video compression}
}

@ARTICLE{10,
  author = {Cucchiara, R. and Grana, C. and Piccardi, M. and Prati, A.},
  title = {Detecting moving objects, ghosts, and shadows in video streams},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2003},
  volume = {25},
  pages = {1337--1342},
  number = {10},
  month = sep,
  abstract = {{Background subtraction methods are widely exploited for moving object
	detection in videos in many applications, such as traffic monitoring,
	human motion capture, and video surveillance. How to correctly and
	efficiently model and update the background model and how to deal
	with shadows are two of the most distinguishing and challenging aspects
	of such approaches. The article proposes a general-purpose method
	that combines statistical assumptions with the object-level knowledge
	of moving objects, apparent objects (ghosts), and shadows acquired
	in the processing of the previous frames. Pixels belonging to moving
	objects, ghosts, and shadows are processed differently in order to
	supply an object-based selective update. The proposed approach exploits
	color information for both background subtraction and shadow detection
	to improve object segmentation and background update. The approach
	proves fast, flexible, and precise in terms of both pixel accuracy
	and reactivity to background changes.}},
  citeulike-article-id = {6033056},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/tpami.2003.1233909},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1233909},
  day = {29},
  doi = {10.1109/tpami.2003.1233909},
  keywords = {background\_image, background\_model, background\_removal, background\_subtraction,
	moving\_objects},
  posted-at = {2009-10-29 18:37:22},
  priority = {2},
  url = {http://dx.doi.org/10.1109/tpami.2003.1233909}
}

@INPROCEEDINGS{1,
  author = {Fall, Kevin},
  title = {A delay-tolerant network architecture for challenged internets},
  booktitle = {Proceedings of the 2003 conference on Applications, technologies,
	architectures, and protocols for computer communications},
  year = {2003},
  series = {SIGCOMM '03},
  pages = {27--34},
  address = {New York, NY, USA},
  publisher = {ACM},
  abstract = {{The highly successful architecture and protocols of today's Internet
	may operate poorly in environments characterized by very long delay
	paths and frequent network partitions. These problems are exacerbated
	by end nodes with limited power or memory resources. Often deployed
	in mobile and extreme environments lacking continuous connectivity,
	many such networks have their own specialized protocols, and do not
	utilize IP. To achieve interoperability between them, we propose
	a network architecture and application interface structured around
	optionally-reliable asynchronous message forwarding, with limited
	expectations of end-to-end connectivity and node resources. The architecture
	operates as an overlay above the transport layers of the networks
	it interconnects, and provides key services such as in-network data
	storage and retransmission, interoperable naming, authenticated forwarding
	and a coarse-grained class of service.}},
  citeulike-article-id = {150388},
  citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=863960},
  citeulike-linkout-1 = {http://dx.doi.org/10.1145/863955.863960},
  doi = {10.1145/863955.863960},
  isbn = {1-58113-735-4},
  keywords = {adhoc, dtn, wireless},
  location = {Karlsruhe, Germany},
  posted-at = {2008-05-23 07:08:55},
  priority = {2},
  url = {http://dx.doi.org/10.1145/863955.863960}
}

@INPROCEEDINGS{5,
  author = {Stephan Hengstler and Hamid Aghajan},
  title = {A Smart Camera Mote Architecture for Distributed Intelligent Surveillance},
  booktitle = {In ACM SenSys Workshop on Distributed Smart Cameras (DSC)},
  year = {2006},
  month = {Oct},
  abstract = {Surveillance is one of the promising applications to which smart camera
	motes forming a vision-enabled network can add increasing levels
	of intelligence. We see a high degree of in-node processing in combination
	with distributed reasoning algorithms as the key enablers for such
	intelligent surveillance systems. To put these systems into practice
	still requires a considerable amount of research ranging from mote
	architectures, pixel-processing algorithms, up to distributed reasoning
	engines. This paper introduces an energy-efficient smart camera mote
	architecture that has been designed with intelligent surveillance
	as the target application in mind. Special attention is given to
	its unique vision system: a low-resolution stereo-vision system continuously
	determines position, range, and size of moving objects entering its
	field of view. This information triggers a color camera module to
	acquire a high-resolution image sub-array containing the object,
	which can be efficiently processed. The paper also presents a basic
	power model that estimates lifetime of our smart camera mote in batterypowered
	operation for intelligent surveillance event processing.},
  owner = {pratyush},
  timestamp = {2013.05.09}
}

@INPROCEEDINGS{3,
  author = {Hengstler, S. and Prashanth, D. and Sufen Fong and Aghajan, H.},
  title = {MeshEye: A Hybrid-Resolution Smart Camera Mote for Applications in
	Distributed Intelligent Surveillance},
  booktitle = {Information Processing in Sensor Networks, 2007. IPSN 2007. 6th International
	Symposium on},
  year = {2007},
  pages = {360-369},
  abstract = {Surveillance is one of the promising applications to which smart camera
	motes forming a vision-enabled network can add increasing levels
	of intelligence. We see a high degree of in-node processing in combination
	with distributed reasoning algorithms as the key enablers for such
	intelligent surveillance systems. To put these systems into practice
	still requires a considerable amount of research ranging from mote
	architectures, pixel-processing algorithms, up to distributed reasoning
	engines. This paper introduces MeshEye, an energy-efficient smart
	camera mote architecture that has been designed with intelligent
	surveillance as the target application in mind. Special attention
	is given to MeshEye's unique vision system: a low-resolution stereo
	vision system continuously determines position, range, and size of
	moving objects entering its field of view. This information triggers
	a color camera module to acquire a high-resolution image sub-array
	containing the object, which can be efficiently processed in subsequent
	stages. It offers reduced complexity, response time, and power consumption
	over conventional solutions. Basic vision algorithms for object detection,
	acquisition, and tracking are described and illustrated on real-
	world data. The paper also presents a basic power model that estimates
	lifetime of our smart camera mote in battery-powered operation for
	intelligent surveillance event processing.},
  doi = {10.1109/IPSN.2007.4379696},
  keywords = {image colour analysis;image resolution;image sensors;intelligent sensors;object
	detection;stereo image processing;surveillance;MeshEye;acquisition;battery-powered
	operation;color camera module;distributed intelligent surveillance;high-resolution
	image sub-array;hybrid-resolution smart camera;low-resolution stereo
	vision system;object detection;pixel-processing algorithm;tracking;vision-enabled
	network;Color;Delay;Energy efficiency;Engines;Intelligent networks;Intelligent
	systems;Machine vision;Smart cameras;Stereo vision;Surveillance;Algorithms;Design;Distributed
	Intelligence;Experimentation;Measurement;Mote Architecture;Performance;Power
	Efficiency;Smart Cameras;Wireless Sensor Networks}
}

@INPROCEEDINGS{13,
  author = {Lo, B. P L and Velastin, S.A.},
  title = {Automatic congestion detection system for underground platforms},
  booktitle = {Intelligent Multimedia, Video and Speech Processing, 2001. Proceedings
	of 2001 International Symposium on},
  year = {2001},
  pages = {158-161},
  abstract = {An automatic monitoring system is proposed in this paper for detecting
	overcrowding conditions in the platforms of underground train services.
	Whenever overcrowding is detected, the system will notify the station
	operators to take appropriate actions to prevent accidents, such
	as people falling off or being pushed onto the tracks. The system
	is designed to use existing closed circuit television (CCTV) cameras
	for acquiring images of the platforms. In order to focus on the passengers
	on the platform, background subtraction and update techniques are
	used. In addition, due to the high variation of brightness on the
	platforms, a variance filter is introduced to optimize the removal
	of background pixels. A multi-layer feed forward neural network was
	developed for classifying the levels of congestion. The system was
	tested with recorded video from the London Bridge station, and the
	testing results were shown to be accurate in identifying overcrowding
	conditions for the unique platform environment},
  doi = {10.1109/ISIMP.2001.925356},
  keywords = {closed circuit television;feedforward neural nets;image processing;monitoring;railways;automatic
	monitoring;closed circuit television;congestion detection system;multi-layer
	feed forward neural network;underground platforms;underground train
	services;variance filter;variation of brightness;Accidents;Brightness;Cameras;Circuits;Computerized
	monitoring;Condition monitoring;Feeds;Filters;System testing;TV}
}

@ARTICLE{14,
  author = {McFarlane, N. J. B. and Schofield, C. P.},
  title = {Segmentation and tracking of piglets in images},
  journal = {Machine Vision and Applications},
  year = {1995},
  volume = {8},
  pages = {187--193},
  number = {3},
  month = may,
  abstract = {{An algorithm was developed for the segmentation and tracking of piglets
	and tested on a 200-image sequence of 10 piglets moving on a straw
	background. The image-capture rate was 1 image/140 ms. The segmentation
	method was a combination of image differencing with respect to a
	median background and a Laplacian operator. The features tracked
	were blob edges in the segmented image. During tracking, the piglets
	were modelled as ellipses initialised on the blobs. Each piglet was
	tracked by searching for blob edges in an elliptical window about
	the piglet's position, which was predicted from its previous two
	positions.}},
  citeulike-article-id = {6017582},
  citeulike-linkout-0 = {http://dx.doi.org/10.1007/bf01215814},
  citeulike-linkout-1 = {http://www.springerlink.com/content/qgl74778617tq121},
  day = {1},
  doi = {10.1007/bf01215814},
  keywords = {approximate\_median, background\_subtraction},
  posted-at = {2009-10-28 06:42:15},
  priority = {4},
  url = {http://dx.doi.org/10.1007/bf01215814}
}

@INPROCEEDINGS{2,
  author = {Pekhteryev, Georgiy and Sahinoglu, Z. and Orlik, P. and Bhatti, G.},
  title = {Image transmission over IEEE 802.15.4 and ZigBee networks},
  booktitle = {Circuits and Systems, 2005. ISCAS 2005. IEEE International Symposium
	on},
  year = {2005},
  pages = {3539-3542 Vol. 4},
  abstract = {An image sensor network platform is developed for testing transmission
	of images over ZigBee networks that support multi-hopping. The ZigBee
	is a low rate and low power networking technology for short range
	communications, and it currently uses IEEE 802.15.4 MAC and PHY layers.
	Both ZigBee networking (NWK) and IEEE 802.15.4 MAC layer protocols
	are implemented on a single M16C microprocessor. Transport layer
	functionalities such as fragmentation and reassembly are performed
	at the application layer, since the ZigBee NWK does not have a fragmentation
	support. The multiple access scheme is CSMA/CA, therefore only the
	best effort multi-hop transmission of JPEG and JPEG-2000 images are
	tested; observations and resulting statistics are presented, and
	open issues are discussed.},
  doi = {10.1109/ISCAS.2005.1465393},
  keywords = {IEEE standards;carrier sense multiple access;code standards;image
	coding;image sensors;microprocessor chips;packet radio networks;quality
	of service;visual communication;wireless LAN;CSMA/CA;IEEE 802.15.4;JPEG-2000;M16C
	microprocessor;MAC layer protocols;PHY layers;ZigBee networks;best
	effort multi-hop transmission;image sensor network platform;image
	transmission;low power networking technology;multi-hopping;short
	range communications;Access protocols;Image communication;Image sensors;Media
	Access Protocol;Microprocessors;Multiaccess communication;Physical
	layer;Statistical analysis;Testing;ZigBee;IEEE 802.15.4;JPEG;JPEG-2000;ZigBee;multi-hop;sensor
	network}
}

@BOOK{6,
  title = {Image and Video Compression for Multimedia Engineering: Fundamentals,
	Algorithms, and Standards},
  publisher = {CRC Press},
  year = {2000},
  author = {Yun Q.Shi and Huifang Sun},
  owner = {pratyush},
  timestamp = {2013.05.10}
}

@INPROCEEDINGS{8,
  author = {Venkatesh Babu, R. and Makur, A.},
  title = {Object-based Surveillance Video Compression using Foreground Motion
	Compensation},
  booktitle = {Control, Automation, Robotics and Vision, 2006. ICARCV '06. 9th International
	Conference on},
  year = {2006},
  pages = {1-6},
  abstract = {Video surveillance is currently one of the most active area of research
	in both academia and industry. Though much work has been done in
	the area of smart surveillance, relatively little work has been reported
	to compress the surveillance videos. In this paper, we propose an
	object based video compression system using foreground motion compensation
	for applications such as archival and transmission of surveillance
	video. The proposed system segments independently moving objects
	from the video and codes them with respect to the previously reconstructed
	frame. The error resulting from object-based motion compensation
	is coded using SA-DCT procedure. The proposed system codes the surveillance
	video using far lesser bits compared to conventional video compression
	techniques},
  doi = {10.1109/ICARCV.2006.345186},
  keywords = {data compression;motion compensation;video coding;video surveillance;SA-DCT
	procedure;foreground motion compensation;object-based surveillance
	video compression;video surveillance;Airports;Cameras;Event detection;Motion
	compensation;Object detection;Object segmentation;Patient monitoring;Road
	safety;Video compression;Video surveillance;Surveillance;Video Compression;Video
	Object Segmentation}
}

@INPROCEEDINGS{12,
  author = {Wren, C. and Azarbayejani, A. and Darrell, T. and Pentland, A.},
  title = {Pfinder: real-time tracking of the human body},
  booktitle = {Automatic Face and Gesture Recognition, 1996., Proceedings of the
	Second International Conference on},
  year = {1996},
  pages = {51-56},
  abstract = {Pfinder is a real-time system for tracking and interpretation of people.
	It runs on a standard SGI Indy computer, and has performed reliably
	on thousands of people in many different physical locations. The
	system uses a multi-class statistical model of color and shape to
	obtain a 2-D representation of head and hands in a wide range of
	viewing conditions. These representations are useful for applications
	such as wireless interfaces, video databases, and low-bandwidth coding,
	without cumbersome wires or attached sensors},
  doi = {10.1109/AFGR.1996.557243},
  keywords = {feature extraction;image sequences;object recognition;optical tracking;real-time
	systems;statistical analysis;Pfinder;SGI Indy computer;hands;head;human
	body;low-bandwidth coding;multi-class statistical model;real-time
	tracking;video databases;wireless interfaces;Cameras;Communication
	system security;Hardware;Head;Humans;Space exploration;Space technology;Video
	compression;Virtual reality;Wireless sensor networks}
}

@INPROCEEDINGS{4,
  author = {Chen Wu and Aghajan, H. and Kleihorst, R.},
  title = {Mapping Vision Algorithms on SIMD Architecture Smart Cameras},
  booktitle = {Distributed Smart Cameras, 2007. ICDSC '07. First ACM/IEEE International
	Conference on},
  year = {2007},
  pages = {27-34},
  abstract = {SIMD (single-instruction multiple-data) processors have demonstrated
	high performance for vector-based image processing, thereby facilitating
	real-time vision applications. However, to fully exploit the advantages
	of the SIMD architecture, implementation of a given vision algorithm
	needs to undergo a mapping from a general purpose CPU programming
	style to a pixel parallel style. This paper describes how part of
	a given gesture analysis algorithm is mapped on a smart camera with
	the SIMD processor to achieve real-time operation. The pixel parallel
	nature of the SIMD processing restricts diversified treatment of
	pixels. Therefore, in this paper we show how to modify the algorithm
	and discuss improvements in the architecture in order to achieve
	the intended functionality. Mapping of background removal, segmentation,
	and labeling functions is described. We also discuss robustness issues
	since the mapping to smart cameras aims for practical, real-time
	applications.},
  doi = {10.1109/ICDSC.2007.4357502},
  keywords = {gesture recognition;image sensors;parallel architectures;SIMD architecture;gesture
	analysis;single-instruction multiple-data processors;smart cameras;vector-based
	image processing;vision algorithm;Algorithm design and analysis;Computer
	architecture;Concurrent computing;Face;Filters;Parallel processing;Parallel
	programming;Performance analysis;Smart cameras;Wireless sensor networks}
}

@INPROCEEDINGS{11,
  author = {Jian Yao and Odobez, J.},
  title = {Multi-Layer Background Subtraction Based on Color and Texture},
  booktitle = {Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference
	on},
  year = {2007},
  pages = {1-8},
  abstract = {In this paper, we propose a robust multi-layer background subtraction
	technique which takes advantages of local texture features represented
	by local binary patterns (LBP) and photometric invariant color measurements
	in RGB color space. LBP can work robustly with respective to light
	variation on rich texture regions but not so efficiently on uniform
	regions. In the latter case, color information should overcome LBP's
	limitation. Due to the illumination invariance of both the LBP feature
	and the selected color feature, the method is able to handle local
	illumination changes such as cast shadows from moving objects. Due
	to the use of a simple layer-based strategy, the approach can model
	moving background pixels with quasi-periodic flickering as well as
	background scenes which may vary over time due to the addition and
	removal of long-time stationary objects. Finally, the use of a cross-bilateral
	filter allows to implicitly smooth detection results over regions
	of similar intensity and preserve object boundaries. Numerical and
	qualitative experimental results on both simulated and real data
	demonstrate the robustness of the proposed method.},
  doi = {10.1109/CVPR.2007.383497},
  issn = {1063-6919},
  keywords = {image colour analysis;image motion analysis;image texture;lighting;video
	signal processing;RGB color space;cross-bilateral filter;illumination
	invariance;layer-based strategy;local binary patterns;local texture
	features;moving background pixels modeling;multilayer background
	subtraction;photometric invariant color measurements;quasi-periodic
	flickering;video stream;Data mining;Filters;Layout;Lighting;Object
	detection;Photometry;Recursive estimation;Robustness;Statistics;Subtraction
	techniques}
}

