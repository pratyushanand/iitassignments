% This file was created with JabRef 2.7b.
% Encoding: UTF-8

@INPROCEEDINGS{10,
  author = {Khalid saeed and Marek Tabedzki and Mariusz Rybink andMarcin Adamski},
  title = {K3M: A Universal Algorithm for Image Skeletonizationand a Review
	of Thinning Techniques},
  booktitle = {Int. J. Appl. Math. Comput. Sci},
  year = {2010},
  volume = {20},
  number = {2},
  pages = {317-335}
}

@ARTICLE{5,
  author = {Barnich, O. and Van Droogenbroeck, M.},
  title = {ViBe: A Universal Background Subtraction Algorithm for Video Sequences},
  journal = {Image Processing, IEEE Transactions on},
  year = {2011},
  volume = {20},
  pages = {1709--1724},
  number = {6},
  month = jun,
  abstract = {{This paper presents a technique for motion detection that incorporates
	several innovative mechanisms. For example, our proposed technique
	stores, for each pixel, a set of values taken in the past at the
	same location or in the neighborhood. It then compares this set to
	the current pixel value in order to determine whether that pixel
	belongs to the background, and adapts the model by choosing randomly
	which values to substitute from the background model. This approach
	differs from those based upon the classical belief that the oldest
	values should be replaced first. Finally, when the pixel is found
	to be part of the background, its value is propagated into the background
	model of a neighboring pixel. We describe our method in full details
	(including pseudo-code and the parameter values used) and compare
	it to other background subtraction techniques. Efficiency figures
	show that our method outperforms recent and proven state-of-the-art
	methods in terms of both computation speed and detection rate. We
	also analyze the performance of a downscaled version of our algorithm
	to the absolute minimum of one comparison and one byte of memory
	per pixel. It appears that even such a simplified version of our
	algorithm performs better than mainstream techniques.}},
  citeulike-article-id = {10548921},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/tip.2010.2101613},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5672785},
  doi = {10.1109/tip.2010.2101613},
  institution = {EVS Broadcast Equip., Seraing, Belgium},
  issn = {1057-7149},
  keywords = {catt},
  posted-at = {2012-09-11 18:10:17},
  priority = {2},
  publisher = {IEEE},
  url = {http://dx.doi.org/10.1109/tip.2010.2101613}
}

@ARTICLE{12,
  author = {Bradski, G.},
  title = {The OpenCV Library},
  journal = {Dr. Dobb's Journal of Software Tools},
  year = {2000},
  citeulike-article-id = {2236121},
  keywords = {bibtex-import},
  posted-at = {2008-01-15 19:21:54},
  priority = {4}
}

@INPROCEEDINGS{13,
  author = {Stauffer, Chris and Grimson, W. E L},
  title = {Adaptive background mixture models for real-time tracking},
  booktitle = {Computer Vision and Pattern Recognition, 1999. IEEE Computer Society
	Conference on.},
  year = {1999},
  volume = {2},
  pages = {-252 Vol. 2},
  abstract = {A common method for real-time segmentation of moving regions in image
	sequences involves “background subtraction“, or thresholding the
	error between an estimate of the image without moving objects and
	the current image. The numerous approaches to this problem differ
	in the type of background model used and the procedure used to update
	the model. This paper discusses modeling each pixel as a mixture
	of Gaussians and using an on-line approximation to update the model.
	The Gaussian, distributions of the adaptive mixture model are then
	evaluated to determine which are most likely to result from a background
	process. Each pixel is classified based on whether the Gaussian distribution
	which represents it most effectively is considered part of the background
	model. This results in a stable, real-time outdoor tracker which
	reliably deals with lighting changes, repetitive motions from clutter,
	and long-term scene changes. This system has been run almost continuously
	for 16 months, 24 hours a day, through rain and snow},
  doi = {10.1109/CVPR.1999.784637},
  issn = {1063-6919},
  keywords = {image segmentation;image sequences;real-time systems;tracking;adaptive
	background mixture models;background subtraction;image sequences;real-time
	segmentation;real-time tracking;thresholding;Adaptive systems;Artificial
	intelligence;Gaussian distribution;Image segmentation;Image sequences;Laboratories;Layout;Robustness;Tracking;Vehicle
	detection}
}


@INPROCEEDINGS{8,
  author = {Chaudhury, S. and Tripathi, S. and Roy, S.D.},
  title = {Parametric video compression using appearance space},
  booktitle = {Pattern Recognition, 2008. ICPR 2008. 19th International Conference
	on},
  year = {2008},
  pages = {1-4},
  abstract = {The novelty of the approach presented in this paper is the unique
	object-based video coding framework for videos obtained from a static
	camera. As opposed to most existing methods, the proposed method
	does not require explicit 2D or 3D models of objects and hence is
	general enough to satisfy the need for varying types of objects in
	the scene. The proposed system detects and tracks an object in the
	scene by learning the appearance model of each object online using
	nontraditional uniform norm based subspace. At the same time the
	object is coded using the projection coefficients to the orthonormal
	basis of the subspace learnt. The tracker incorporates a predictive
	framework based upon Kalman filter for predicting the five motion
	parameters. The proposed method shows substantially better compression
	than MPEG2 based coding with almost no additional complexity.},
  doi = {10.1109/ICPR.2008.4761652},
  issn = {1051-4651},
  keywords = {Kalman filters;cameras;data compression;object detection;video coding;Kalman
	filter;MPEG2 based coding;appearance space;object detection;object
	tracking;parametric video compression;projection coefficients;static
	camera;uniform norm based subspace;unique object-based video coding;Cameras;Encoding;Image
	coding;Image sequences;Layout;Object detection;Tracking;Transform
	coding;Video coding;Video compression}
}

@ARTICLE{1,
  author = {Cucchiara, R. and Grana, C. and Piccardi, M. and Prati, A.},
  title = {Detecting moving objects, ghosts, and shadows in video streams},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2003},
  volume = {25},
  pages = {1337--1342},
  number = {10},
  month = sep,
  abstract = {{Background subtraction methods are widely exploited for moving object
	detection in videos in many applications, such as traffic monitoring,
	human motion capture, and video surveillance. How to correctly and
	efficiently model and update the background model and how to deal
	with shadows are two of the most distinguishing and challenging aspects
	of such approaches. The article proposes a general-purpose method
	that combines statistical assumptions with the object-level knowledge
	of moving objects, apparent objects (ghosts), and shadows acquired
	in the processing of the previous frames. Pixels belonging to moving
	objects, ghosts, and shadows are processed differently in order to
	supply an object-based selective update. The proposed approach exploits
	color information for both background subtraction and shadow detection
	to improve object segmentation and background update. The approach
	proves fast, flexible, and precise in terms of both pixel accuracy
	and reactivity to background changes.}},
  citeulike-article-id = {6033056},
  citeulike-linkout-0 = {http://dx.doi.org/10.1109/tpami.2003.1233909},
  citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1233909},
  day = {29},
  doi = {10.1109/tpami.2003.1233909},
  keywords = {background\_image, background\_model, background\_removal, background\_subtraction,
	moving\_objects},
  posted-at = {2009-10-29 18:37:22},
  priority = {2},
  url = {http://dx.doi.org/10.1109/tpami.2003.1233909}
}

@INPROCEEDINGS{6,
  author = {Venkatesh Babu, R. and Makur, A.},
  title = {Object-based Surveillance Video Compression using Foreground Motion
	Compensation},
  booktitle = {Control, Automation, Robotics and Vision, 2006. ICARCV '06. 9th International
	Conference on},
  year = {2006},
  pages = {1-6},
  abstract = {Video surveillance is currently one of the most active area of research
	in both academia and industry. Though much work has been done in
	the area of smart surveillance, relatively little work has been reported
	to compress the surveillance videos. In this paper, we propose an
	object based video compression system using foreground motion compensation
	for applications such as archival and transmission of surveillance
	video. The proposed system segments independently moving objects
	from the video and codes them with respect to the previously reconstructed
	frame. The error resulting from object-based motion compensation
	is coded using SA-DCT procedure. The proposed system codes the surveillance
	video using far lesser bits compared to conventional video compression
	techniques},
  doi = {10.1109/ICARCV.2006.345186},
  keywords = {data compression;motion compensation;video coding;video surveillance;SA-DCT
	procedure;foreground motion compensation;object-based surveillance
	video compression;video surveillance;Airports;Cameras;Event detection;Motion
	compensation;Object detection;Object segmentation;Patient monitoring;Road
	safety;Video compression;Video surveillance;Surveillance;Video Compression;Video
	Object Segmentation}
}


@INPROCEEDINGS{9,
  author = {Jianhao Ding and Yigang Wang and Lingyun Yu},
  title = {Extraction of Human Body Skeleton Based on Silhouette Images},
  booktitle = {Education Technology and Computer Science (ETCS), 2010 Second International
	Workshop on},
  year = {2010},
  volume = {1},
  pages = {71-74},
  abstract = {Skeleton extraction is essential for general shape representation.
	A typical skeletonization algorithm should obtain the ability to
	preserve original object's topological and hierarchical properties.
	However, most of current methods are high memory cost, computationally
	intensive, and also require complex data structures. In this paper,
	we propose an efficient and accurate skeletonization method for the
	skeleton feature points extracted from human body based on silhouette
	images. First, the gradient of distance transform is used to detect
	critical points inside the foreground. Then, we converge and simplify
	critical points in order to generate the most important and elegant
	skeleton feature points. Finally, we present an algorithm which connects
	the skeleton feature points and estimates the position of skeleton
	joints.},
  doi = {10.1109/ETCS.2010.241},
  keywords = {data structures;feature extraction;image representation;image thinning;complex
	data structures;human body skeleton extraction;memory cost;shape
	representation;silhouette images;skeletonization algorithm;Data mining;Euclidean
	distance;Feature extraction;Humans;Image converters;Iterative algorithms;Joints;Motion
	analysis;Shape;Skeleton;Feature Detection;Joints estimation;Skeletonization}
}

@INPROCEEDINGS{4,
  author = {Dollar, Piotr and Belongie, Serge and Perona, Pietro},
  title = {The Fastest Pedestrian Detector in the West},
  booktitle = {Procedings of the British Machine Vision Conference 2010},
  year = {2010},
  pages = {68.1--68.11},
  publisher = {British Machine Vision Association},
  citeulike-article-id = {11240224},
  citeulike-linkout-0 = {http://dx.doi.org/10.5244/c.24.68},
  doi = {10.5244/c.24.68},
  isbn = {1-901725-40-5},
  location = {Aberystwyth},
  posted-at = {2012-09-13 23:31:29},
  priority = {2},
  url = {http://dx.doi.org/10.5244/c.24.68}
}

@INPROCEEDINGS{11,
  author = {Fujiyoshi, H. and Lipton, A.J.},
  title = {Real-time human motion analysis by image skeletonization},
  booktitle = {Applications of Computer Vision, 1998. WACV '98. Proceedings., Fourth
	IEEE Workshop on},
  year = {1998},
  pages = {15-21},
  abstract = {In this paper a process is described for analysing the motion of a
	human target in a video stream. Moving targets are detected and their
	boundaries extracted. From these, a “star” skeleton is produced.
	Two motion cues are determined from this skeletonization: body posture,
	and cyclic motion of skeleton segments. These cues are used to determine
	human activities such as walking or running, and even potentially,
	the target's gait. Unlike other methods, this does not require an
	a priori human model, or a large number of “pixels on target”. Furthermore,
	it is computationally inexpensive, and thus ideal for real-world
	video applications such as outdoor video surveillance},
  doi = {10.1109/ACV.1998.732852},
  keywords = {image thinning;video signal processing;body posture;cyclic motion;human
	motion analysis;human target;image skeletonization;motion cues;running;video
	surveillance;walking;Algorithm design and analysis;Data mining;Humans;Image
	motion analysis;Motion analysis;Motion detection;Optical computing;Optical
	sensors;Skeleton;Streaming media}
}

@ARTICLE{7,
  author = {Tsung-Han Tsai and Chung-Yuan Lin},
  title = {Exploring Contextual Redundancy in Improving Object-Based Video Coding
	for Video Sensor Networks Surveillance},
  journal = {Multimedia, IEEE Transactions on},
  year = {2012},
  volume = {14},
  pages = {669-682},
  number = {3},
  abstract = {In recent years, intelligent video surveillance attempts to provide
	content analysis tools to understand and predict the actions via
	video sensor networks (VSN) for automated wide-area surveillance.
	In this emerging network, visual object data is transmitted through
	different devices to adapt to the needs of the specific content analysis
	task. Therefore, they raise a new challenge for video delivery: how
	to efficiently transmit visual object data to various devices such
	as storage device, content analysis server, and remote client server
	through the network. Object-based video encoder can be used to reduce
	transmission bandwidth with minor quality loss. However, the involved
	motion-compensated technique often leads to high computational complexity
	and consequently increases the cost of VSN. In this paper, contextual
	redundancy associated with background and foreground objects in a
	scene is explored. A scene analysis method is proposed to classify
	macroblocks (MBs) by type of contextual redundancy. The motion search
	is only performed on the specific type of context of MB which really
	involves salient motion. To facilitate the encoding by context of
	MB, an improved object-based coding architecture, namely dual-closed-loop
	encoder, is derived. It encodes the classified context of MB in an
	operational rate-distortion-optimized sense. The experimental results
	show that the proposed coding framework can achieve higher coding
	efficiency than MPEG-4 coding and related object-based coding approaches,
	while significantly reducing coding complexity.},
  doi = {10.1109/TMM.2011.2180705},
  issn = {1520-9210},
  keywords = {computational complexity;image classification;image sensors;motion
	compensation;video coding;video surveillance;MPEG-4 coding;VSN;coding
	complexity reduction;computational complexity;content analysis server;content
	analysis tools;contextual redundancy;dual-closed-loop encoder;intelligent
	video surveillance;macroblocks classification;minor quality loss;motion-compensated
	technique;object-based coding architecture;object-based video coding;object-based
	video encoder;operational rate-distortion-optimized sense;remote
	client server;scene analysis method;storage device;transmission bandwidth
	reduction;video delivery;video sensor networks surveillance;visual
	object data transmission;Context;Encoding;Lighting;Redundancy;Surveillance;Transform
	coding;Visualization;Contextual redundancy coding;intelligent video
	surveillance;object-based video coding;operational rate-distortion
	theory;visual sensor network}
}

@INPROCEEDINGS{2,
  author = {Viola, P. and Jones, M.J. and Snow, D.},
  title = {Detecting pedestrians using patterns of motion and appearance},
  booktitle = {Computer Vision, 2003. Proceedings. Ninth IEEE International Conference
	on},
  year = {2003},
  pages = {734-741 vol.2},
  abstract = {This paper describes a pedestrian detection system that integrates
	image intensity information with motion information. We use a detection
	style algorithm that scans a detector over two consecutive frames
	of a video sequence. The detector is trained (using AdaBoost) to
	take advantage of both motion and appearance information to detect
	a walking person. Past approaches have built detectors based on appearance
	information, but ours is the first to combine both sources of information
	in a single detector. The implementation described runs at about
	4 frames/second, detects pedestrians at very small scales (as small
	as 20×15 pixels), and has a very low false positive rate. Our approach
	builds on the detection work of Viola and Jones. Novel contributions
	of this paper include: i) development of a representation of image
	motion which is extremely efficient, and ii) implementation of a
	state of the art pedestrian detection system which operates on low
	resolution images under difficult conditions (such as rain and snow).},
  doi = {10.1109/ICCV.2003.1238422},
  keywords = {computer vision;feature extraction;image motion analysis;image representation;image
	resolution;image sequences;object detection;15 pixels;20 pixels;300
	pixels;AdaBoost;detection style algorithm;detector scanning;image
	intensity information;image motion representation;low resolution
	images;motion appearance;motion information;motion patterns;pedestrian
	detection;video sequence frames;walking person;Detectors;Face detection;Humans;Image
	resolution;Motion analysis;Motion detection;Object detection;Pattern
	recognition;Rain;Snow}
}

@INPROCEEDINGS{3,
  author = {Jian Yao and Odobez, J.},
  title = {Multi-Layer Background Subtraction Based on Color and Texture},
  booktitle = {Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference
	on},
  year = {2007},
  pages = {1-8},
  abstract = {In this paper, we propose a robust multi-layer background subtraction
	technique which takes advantages of local texture features represented
	by local binary patterns (LBP) and photometric invariant color measurements
	in RGB color space. LBP can work robustly with respective to light
	variation on rich texture regions but not so efficiently on uniform
	regions. In the latter case, color information should overcome LBP's
	limitation. Due to the illumination invariance of both the LBP feature
	and the selected color feature, the method is able to handle local
	illumination changes such as cast shadows from moving objects. Due
	to the use of a simple layer-based strategy, the approach can model
	moving background pixels with quasi-periodic flickering as well as
	background scenes which may vary over time due to the addition and
	removal of long-time stationary objects. Finally, the use of a cross-bilateral
	filter allows to implicitly smooth detection results over regions
	of similar intensity and preserve object boundaries. Numerical and
	qualitative experimental results on both simulated and real data
	demonstrate the robustness of the proposed method.},
  doi = {10.1109/CVPR.2007.383497},
  issn = {1063-6919},
  keywords = {image colour analysis;image motion analysis;image texture;lighting;video
	signal processing;RGB color space;cross-bilateral filter;illumination
	invariance;layer-based strategy;local binary patterns;local texture
	features;moving background pixels modeling;multilayer background
	subtraction;photometric invariant color measurements;quasi-periodic
	flickering;video stream;Data mining;Filters;Layout;Lighting;Object
	detection;Photometry;Recursive estimation;Robustness;Statistics;Subtraction
	techniques}
}

@INPROCEEDINGS{15,
  author = {Stauffer, Chris and Grimson, W. E L},
  title = {Adaptive background mixture models for real-time tracking},
  booktitle = {Computer Vision and Pattern Recognition, 1999. IEEE Computer Society
	Conference on.},
  year = {1999},
  volume = {2},
  pages = {-252 Vol. 2},
  abstract = {A common method for real-time segmentation of moving regions in image
	sequences involves “background subtraction“, or thresholding the
	error between an estimate of the image without moving objects and
	the current image. The numerous approaches to this problem differ
	in the type of background model used and the procedure used to update
	the model. This paper discusses modeling each pixel as a mixture
	of Gaussians and using an on-line approximation to update the model.
	The Gaussian, distributions of the adaptive mixture model are then
	evaluated to determine which are most likely to result from a background
	process. Each pixel is classified based on whether the Gaussian distribution
	which represents it most effectively is considered part of the background
	model. This results in a stable, real-time outdoor tracker which
	reliably deals with lighting changes, repetitive motions from clutter,
	and long-term scene changes. This system has been run almost continuously
	for 16 months, 24 hours a day, through rain and snow},
  doi = {10.1109/CVPR.1999.784637},
  issn = {1063-6919},
  keywords = {image segmentation;image sequences;real-time systems;tracking;adaptive
	background mixture models;background subtraction;image sequences;real-time
	segmentation;real-time tracking;thresholding;Adaptive systems;Artificial
	intelligence;Gaussian distribution;Image segmentation;Image sequences;Laboratories;Layout;Robustness;Tracking;Vehicle
	detection}
}

@INPROCEEDINGS{19,
  author = {Yao, Jian and Odobez, Jean-Marc},
  title = {Fast human detection from videos using covariance features},
  booktitle = {European Conference on Computer Vision, workshop on Visual Surveillance
	(ECCV-VS)},
  year = {2008},
  month = {10},
  abstract = {In this paper, we present a fast method to detect humans from videos
	captured in surveillance applications. It is based on a cascade of
	LogitBoost classifiers relying on features mapped from the Riemanian
	manifold of region covariance matrices computed from input image
	features. The method was extended in several ways. First, as the
	mapping process is slow for high dimensional feature space, we propose
	to select weak classifiers based on subsets of the complete image
	feature space. In addition, we propose to combine these sub-matrix
	covariance features with the means of the image features computed
	within the same subwindow, which are readily available from the covariance
	extraction process. Finally, in the context of video acquired with
	stationary cameras, we propose to fuse image features from the spatial
	and temporal domains in order to jointly learn the correlation between
	appearance and foreground information based on background subtraction.
	Our method evaluated on a large set of videos coming from several
	databases (CAVIAR, PETS, ...,',','),
	
	and can process from 5 to 20 frames/sec (for a 384x288 video) while
	achieving similar or better performance than existing methods.},
  crossref = {11},
  location = {Marseille},
  pdf = {http://publications.idiap.ch/downloads/papers/2008/Yao_ECCV-VS_2008.pdf},
  projects = {Idiap, CARETAKER}
}


@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

